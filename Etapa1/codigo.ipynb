{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PROYECTO 1 - ETAPA 1\n",
    "**GRUPO 2:** \\\n",
    "Juana Mejía 202021512\\\n",
    "Daniela Espinosa 202022615 \\\n",
    "Panblo Ortega"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Carga de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_excel('ODScat_345.xlsx')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perfilamiento de los Datos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estadisticas descriptivas\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El describe nos da información sobre la única variable numérica, sin embargo, debemos recordar que esta variable es una categoria codificada. Por este motivo la media, desviación y percentiles no resultan muy útiles. El count nos dice el número de valores que tiene la columna."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Informacion del data set\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como podemos ver, el dataframe no tiene valores nulos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Análisis de duplicados\n",
    "duplicates = df.duplicated().sum()\n",
    "print(f\"\\nNúmero de registros duplicados: {duplicates}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tampoco tiene duplicados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "levels = df.apply(pd.Series.unique)\n",
    "num_levels = df.apply(pd.Series.nunique)\n",
    "\n",
    "print(levels)\n",
    "print(num_levels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La columna sdg contiene valores numéricos y tiene tres ategorías: [3, 4, 5]\n",
    "Todos los valores de Textos_espanol son de texto y son diferentes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Limpieza de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalización de textos\n",
    "data['Textos_espanol'] = data['Textos_espanol'].str.lower()\n",
    "data['Textos_espanol'] = data['Textos_espanol'].str.replace('[^\\w\\s]', '', regex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vectorización de textos\n",
    "Encontrar el número óptimo del máximo de features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import make_scorer, accuracy_score\n",
    "from nltk.corpus import stopwords\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Cargar las palabras de parada en español\n",
    "spanish_stopwords = stopwords.words('spanish')\n",
    "\n",
    "# Definir el pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(stop_words=spanish_stopwords)),\n",
    "    ('clf', MultinomialNB())\n",
    "])\n",
    "\n",
    "# Definir el rango de parámetros para buscar\n",
    "param_grid = {\n",
    "    'tfidf__max_features': [1000,2000,3000,4000,5000,6000,7000,8000,9000,10000]\n",
    "}\n",
    "\n",
    "# Configurar el GridSearchCV\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring=make_scorer(accuracy_score))\n",
    "\n",
    "# Ajustar el modelo con el GridSearchCV\n",
    "grid_search.fit(data['Textos_espanol'], data['sdg'])\n",
    "\n",
    "# Mejor número de max_features\n",
    "best_max_features = grid_search.best_params_['tfidf__max_features']\n",
    "print(f\"Mejor número de max_features: {best_max_features}\")\n",
    "\n",
    "# Obtener los resultados de precisión media para cada valor de max_features\n",
    "mean_scores = grid_search.cv_results_['mean_test_score']\n",
    "\n",
    "# Graficar los resultados\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(param_grid['tfidf__max_features'], mean_scores, marker='o', linestyle='-')\n",
    "plt.xlabel('Número de max_features')\n",
    "plt.ylabel('Precisión media')\n",
    "plt.title('Rendimiento del modelo según max_features')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se utilizó cross-validation para elegir el número optimo de características del modelo después de vectorizar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Vectorizar el texto**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Cargar las palabras de parada en español\n",
    "spanish_stopwords = stopwords.words('spanish')\n",
    "\n",
    "# Vectorización de textos\n",
    "vectorizer = TfidfVectorizer(stop_words=spanish_stopwords, max_features=best_max_features)\n",
    "text_vectors = vectorizer.fit_transform(data['Textos_espanol'])\n",
    "text_data = pd.DataFrame(text_vectors.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "# Concatenar con el dataset original\n",
    "data = pd.concat([data.drop(columns=['Textos_espanol']), text_data], axis=1)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se vectorizó el texto con base en la cantidad óptima de características obtenidas anteriormente. El resultado es un dataframe con la misma cantidad de filas pero 8000 columnas de características, una por palabra. Cada una de estas columnas es un vector que depende de la relevancia de la palabra."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Separar los datos en train y test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Dividir el conjunto de datos en características y variable objetivo\n",
    "X = data.drop(columns=['sdg'])\n",
    "Y = data[['sdg']]\n",
    "\n",
    "\n",
    "mapping = {3: 0, 4: 1, 5: 2}\n",
    "Y = Y['sdg'].map(mapping)\n",
    "\n",
    "\n",
    "# Dividir los datos en conjunto de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separación de los datos en variable objetivo y características y mapping de las categorias (3,4,5) a (0,1,2) respectivamente para poder utilizar XGboost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementación de modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# Definir los modelos en un diccionario\n",
    "models = {\n",
    "    'KNN': KNeighborsClassifier(),\n",
    "    'Naive Bayes': MultinomialNB(),\n",
    "    'Random Forest': RandomForestClassifier(),\n",
    "    'XGboost': XGBClassifier(),\n",
    "    'NeuralNet': MLPClassifier()\n",
    "}\n",
    "\n",
    "# Recorrer los modelos\n",
    "for name, model in models.items():\n",
    "       \n",
    "    # Entrenar el modelo\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predecir con el modelo entrenado\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Evaluar el modelo\n",
    "    print(f\"\\n{name}:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "    # Matriz de confusión\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['sdg3', 'sdg4', 'sdg5'], yticklabels=['sdg3', 'sdg4', 'sdg5'])\n",
    "    plt.title(f'{name} - Confusion Matrix')\n",
    "    plt.ylabel('True Labels')\n",
    "    plt.xlabel('Predicted Labels')\n",
    "    plt.show()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se implementaron 5 modelos diferentes, por ahora los que más prometen son Naive Bayes y la red neuronal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construcción del modelo con optimización de hiperparámetros ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "\n",
    "\n",
    "\n",
    "# Definir los hiperparámetros a optimizar para cada modelo\n",
    "param_grids = {\n",
    "    'KNN': {\n",
    "        'n_neighbors': [3, 5, 7, 9],\n",
    "        'weights': ['uniform', 'distance'],\n",
    "        'metric': ['minkowski','euclidean', 'manhattan']\n",
    "    },\n",
    "    'Naive Bayes': {\n",
    "        'alpha': [0.1, 0.5, 1.0, 2.0]\n",
    "    },\n",
    "    'Random Forest': {\n",
    "        'criterion':['gini', 'entropy'],\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'max_depth': [1, 10, 20, 30],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4],\n",
    "        'bootstrap': [True, False]\n",
    "    },\n",
    "    'XGboost': {\n",
    "        'n_estimators': [100, 200],\n",
    "        'learning_rate': [0.01, 0.1, 0.2],\n",
    "        'max_depth': [3, 5, 7],\n",
    "        'min_child_weight': [1, 3, 5],\n",
    "        'subsample': [0.8, 1.0],\n",
    "        'colsample_bytree': [0.8, 1.0]\n",
    "    },\n",
    "    'NeuralNet': {\n",
    "        'hidden_layer_sizes': [(50,), (100,), (50, 50)],\n",
    "        'activation': ['tanh', 'relu'],\n",
    "        'solver': ['adam', 'sgd'],\n",
    "        'alpha': [0.0001, 0.001, 0.01],\n",
    "        'learning_rate': ['constant', 'adaptive']\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# Recorrer los modelos y realizar la búsqueda de hiperparámetros\n",
    "for name, model in models.items():\n",
    "    \n",
    "    # Seleccionar el tipo de búsqueda de hiperparámetros (GridSearchCV o RandomizedSearchCV)\n",
    "    if name in ['KNN', 'Naive Bayes']:\n",
    "        search = GridSearchCV(model, param_grids[name], cv=5, scoring='accuracy', n_jobs=-1)\n",
    "    else:\n",
    "        search = RandomizedSearchCV(model, param_grids[name], n_iter=15, cv=5, scoring='accuracy', n_jobs=-1, random_state=42)\n",
    "    \n",
    "    # Entrenar el modelo con búsqueda de hiperparámetros\n",
    "    search.fit(X_train, y_train)\n",
    "    \n",
    "    # Mejor modelo encontrado\n",
    "    best_model = search.best_estimator_\n",
    "    \n",
    "    # Predecir con el mejor modelo\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    \n",
    "    # Evaluar el mejor modelo\n",
    "    print(f\"\\n{name} Classifier Report (Best Model):\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    # Matriz de confusión\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['sdg3', 'sdg4', 'sdg5'], yticklabels=['sdg3', 'sdg4', 'sdg5'])\n",
    "    plt.title(f'{name} - Confusion Matrix (Best Model)')\n",
    "    plt.ylabel('True Labels')\n",
    "    plt.xlabel('Predicted Labels')\n",
    "    plt.show()\n",
    "    \n",
    "    # Mostrar los mejores hiperparámetros\n",
    "    print(f\"Best hyperparameters for {name}: {search.best_params_}\")\n",
    "\n",
    "    print('Exactitud sobre test: %.2f' % accuracy_score(y_test, y_pred)) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizando GridSearch y RandomizedSearch para la optimización de hiperparámetros se lograron mejorar las métricas de calidad de los modelos. El mejor modelo resultante es la red neuronal y en segundo lugar Naive Bayes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Modelos con hiperparametros dados:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esto se hizo en caso de perder el proceso de optimización de la parte superior, ya que se demoró horas en realizar el cross-validation para todos los modelos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import pandas as pd\n",
    "# Definir los modelos en un diccionario\n",
    "models = {'KNN': KNeighborsClassifier(n_neighbors= 9,weights='distance',metric='minkowski'),\n",
    "    'Naive Bayes': MultinomialNB(alpha= 1.0),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators= 200, criterion = 'entropy',max_depth= 20, min_samples_split= 5, min_samples_leaf= 2, bootstrap= False),\n",
    "    'XGboost': XGBClassifier(subsample=1.0, n_estimators= 200, min_child_weight= 1, max_depth= 5, learning_rate= 0.1, colsample_bytree= 1.0),\n",
    "    'NeuralNet': MLPClassifier(solver= 'adam',  alpha= 0.01,learning_rate='constant', hidden_layer_sizes= (100,),activation= 'tanh')}\n",
    "\n",
    "# Recorrer los modelos\n",
    "for name, model in models.items():\n",
    "       \n",
    "    # Entrenar el modelo\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predecir con el modelo entrenado\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Evaluar el modelo\n",
    "    print(f\"\\n{name}:\")\n",
    "    report = classification_report(y_test, y_pred, target_names=['sdg3', 'sdg4', 'sdg5'], output_dict=True)\n",
    "\n",
    "# Convertir el reporte a un DataFrame para mejor manipulación\n",
    "    report_df = pd.DataFrame(report).transpose()\n",
    "# Formatear el DataFrame para mostrar 4 cifras decimales\n",
    "    report_df = report_df.round(4)\n",
    "    print(report_df)\n",
    "\n",
    "    # Matriz de confusión\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['sdg3', 'sdg4', 'sdg5'], yticklabels=['sdg3', 'sdg4', 'sdg5'])\n",
    "    plt.title(f'{name} - Confusion Matrix')\n",
    "    plt.ylabel('True Labels')\n",
    "    plt.xlabel('Predicted Labels')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Datos\n",
    "labels = ['KNN', 'Naive Bayes', 'Random Forest', 'XGBoost', 'ANN']\n",
    "f1_scores = [95.92, 97.53, 96.55, 95.19, 97.65]\n",
    "\n",
    "# Configuración del gráfico\n",
    "x = np.arange(len(labels))  # Etiquetas para cada algoritmo\n",
    "width = 0.4  # Ancho de las barras\n",
    "\n",
    "# Crear la figura y los ejes\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Graficar las barras para F1-score\n",
    "rects1 = ax.bar(x, f1_scores, width, label='F1-score', color='#a21942')\n",
    "\n",
    "# Añadir etiquetas y título con tamaños de fuente aumentados\n",
    "ax.set_xlabel('Algoritmo', fontsize=14)\n",
    "ax.set_ylabel('F1-score (%)', fontsize=14)\n",
    "ax.set_title('Comparación de F1-score por Algoritmo', fontsize=16)\n",
    "\n",
    "# Personalizar los ticks del eje x y del eje y\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(labels, fontsize=12)\n",
    "ax.tick_params(axis='y', labelsize=12)\n",
    "\n",
    "# Limitar el eje y entre 90 y 100\n",
    "ax.set_ylim(90, 100)\n",
    "\n",
    "# Añadir los valores encima de cada barra\n",
    "for rect in rects1:\n",
    "    height = rect.get_height()\n",
    "    ax.text(\n",
    "        rect.get_x() + rect.get_width() / 2.,  # Posición en el eje x\n",
    "        height,  # Altura (posición en el eje y)\n",
    "        f'{height:.2f}',  # Formato del texto (F1-score con 2 decimales)\n",
    "        ha='center', va='bottom', fontsize=12  # Aumentar tamaño del texto sobre las barras\n",
    "    )\n",
    "\n",
    "# Mostrar gráfico\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se puede observar el mejor algoritmo es ANN con un f1-score de 97.65. Sin embargo, en el momento de hacer la recomendación al cliente se preferiría utilizar Naive Bayes debido a que es un modelo más eficiente computacionalmente y fácil de optimizar debido a que tiene un solo hiperparámetro. Adicionalmente es un modelo interpretable a diferencia de la red neuronal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "model=MultinomialNB(alpha= 1.0)\n",
    "\n",
    "\n",
    "# Entrenar el modelo\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predecir con el modelo entrenado\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Matriz de confusión\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['sdg3', 'sdg4', 'sdg5'], yticklabels=['sdg3', 'sdg4', 'sdg5'])\n",
    "plt.title('NB - Confusion Matrix')\n",
    "plt.ylabel('True Labels')\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Interpretabilidad\n",
    "feature_log_probs = model.feature_log_prob_\n",
    "\n",
    "top_n = 3\n",
    "for i, class_label in enumerate(model.classes_):\n",
    "    sorted_idx = np.argsort(feature_log_probs[i])[::-1]\n",
    "    print(\"-------------------------------------------------\")\n",
    "    print(f\"Top {top_n} palabras más importantes para el objetivo de desarrollo sostenible {class_label+3}:\")\n",
    "    for idx in sorted_idx[:top_n]:\n",
    "        print(f\"{X_train.columns[idx]}: {np.exp(feature_log_probs[i][idx]):.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se puede interpretar el modelo para extraer las palabras que tienen más relevancia para el modelo en la toma de decisiones. Como se puede observar se ajustan muy bien a cada ODS."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
